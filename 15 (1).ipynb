{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a04d2c-a9bf-459f-acd2-bc09d69c52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:1\n",
    "Linear regression and logistic regression are both regression techniques used in different types of data analysis, but they serve distinct purposes and are suited for different types of data and problems.\n",
    "\n",
    "Linear Regression:\n",
    "Linear regression is used to model the relationship between a dependent variable (target) and one or more independent variables (predictors) in a continuous, numeric context. It aims to find the best-fitting linear equation that predicts the numerical value of the dependent variable based on the values of the independent variables.\n",
    "\n",
    "Logistic Regression:\n",
    "Logistic regression, despite its name, is a classification algorithm used for predicting binary outcomes (0 or 1). It models the probability that a given input point belongs to a certain class. Instead of predicting a continuous value, it predicts the probability of a certain event occurring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe16bc7-704c-4097-ac8e-82c6a341d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:2\n",
    "The cost function used in logistic regression is the logarithmic loss, also known as the log loss or cross-entropy loss. The goal of logistic regression is to find the best set of parameters (coefficients) that maximizes the likelihood of the observed outcomes given the input features. The log loss function quantifies how well the predicted probabilities match the actual binary labels.\n",
    "\n",
    "Log Loss = -[y * log(p) + (1 - y) * log(1 - p)]\n",
    "\n",
    "Cost Function = (1/m) * Σ [-y * log(p) - (1 - y) * log(1 - p)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cbca12-28ef-4fa0-9bc7-1259955a02d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:3\n",
    "Regularization is a technique used in machine learning, including logistic regression, to prevent overfitting by adding a penalty term to the model's cost function. Overfitting occurs when a model fits the training data too closely, capturing noise and random fluctuations in the data instead of the underlying patterns. Regularization helps control the complexity of the model and encourages it to generalize well to new, unseen data.\n",
    "\n",
    "In the context of logistic regression, two common types of regularization are used: L1 regularization (Lasso regularization) and L2 regularization (Ridge regularization).\n",
    "\n",
    "L1 Regularization (Lasso):\n",
    "L1 regularization adds the sum of the absolute values of the coefficients as a penalty term to the cost function. \n",
    "\n",
    "Regularized Cost = -[y * log(p) + (1 - y) * log(1 - p)] + λ * Σ|β|\n",
    "\n",
    "\n",
    "L2 Regularization (Ridge):\n",
    "L2 regularization adds the sum of the squared values of the coefficients as a penalty term to the cost function.\n",
    "Regularized Cost = -[y * log(p) + (1 - y) * log(1 - p)] + λ * Σβ^2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6f55a-3154-42c7-a631-f3d1c3338953",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:4\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation that illustrates the performance of a classification model, such as a logistic regression model, across different levels of classification threshold.\n",
    "\n",
    "he ROC curve is created by plotting the true positive rate (sensitivity) against the false positive rate (1-specificity) as the threshold for classifying positive instances is varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c2960-1ebc-4ece-b335-fd9e437c4680",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:5\n",
    "\n",
    "Feature selection is the process of choosing a subset of relevant features (predictor variables) from the original set of features to improve the performance of a model, such as logistic regression. It helps reduce overfitting, improve model interpretability, and speed up training time. Here are some common techniques for feature selection in logistic regression:\n",
    "\n",
    "**1. Univariate Feature Selection:\n",
    "\n",
    "This method involves evaluating each feature independently using statistical tests like chi-squared test, ANOVA, or mutual information. Features with the highest test statistics or information gain are selected.\n",
    "This approach is simple but doesn't consider interactions between features.\n",
    "2. Recursive Feature Elimination (RFE):\n",
    "\n",
    "RFE is an iterative technique that starts with all features and repeatedly removes the least important feature based on a model's performance.\n",
    "The model is trained and evaluated after each feature is removed, and the process continues until a desired number of features is reached.\n",
    "3. L1 Regularization (Lasso):\n",
    "\n",
    "L1 regularization adds a penalty based on the absolute values of the coefficients to the logistic regression cost function.\n",
    "It can drive some coefficients to exactly zero, effectively performing automatic feature selection.\n",
    "4. Tree-Based Methods:\n",
    "\n",
    "Tree-based methods like decision trees and random forests can be used to rank features based on their importance in splitting nodes.\n",
    "Features with higher importance scores are considered more relevant and are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea48ef6-ce52-4bb3-8e14-0dc41527f57c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
